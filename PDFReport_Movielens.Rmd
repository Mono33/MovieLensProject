---
title: "Capstone1: MovieLens project"
author: "Louis Mono"
date: "March 2019"

output:
  pdf_document:
    keep_tex: yes
    latex_engine: xelatex
    number_sections: yes
    toc: yes
    toc_depth: 4
    fig_caption: yes
urlcolor: blue
linkcolor: red
header-includes:
- \usepackage{graphicx}
- \usepackage{float}
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{xcolor}
---

```{r echo=FALSE,warning=FALSE}
 library(knitr)
  opts_chunk$set(fig.path='figure/graphics-', 
                 cache.path='cache/graphics-', 
                 fig.align='center',
                 external=TRUE,
                 echo=TRUE,
                 warning=FALSE,
                 fig.pos='H'
                )
#  a4width<- 8.3
#  a4height<- 11.7
```

```{r include=FALSE}
#remove all # (comments)
knitr::opts_chunk$set(comment = NA)
```

```{r include=FALSE}
# A Prefix nulling hook.

# Make sure to keep the default for normal processing.
default_output_hook <- knitr::knit_hooks$get("output")

# Output hooks handle normal R console output.
knitr::knit_hooks$set( output = function(x, options) {

  comment <- knitr::opts_current$get("comment")
  if( is.na(comment) ) comment <- ""
  can_null <- grepl( paste0( comment, "\\s*\\[\\d?\\]" ),
                     x, perl = TRUE)
  do_null <- isTRUE( knitr::opts_current$get("null_prefix") )
  if( can_null && do_null ) {
    # By default R print output aligns at the right brace.
    align_index <- regexpr( "\\]", x )[1] - 1
    # Two cases: start or newline
    re <- paste0( "^.{", align_index, "}\\]")
    rep <- comment
    x <- gsub( re, rep,  x )
    re <- paste0( "\\\n.{", align_index, "}\\]")
    rep <- paste0( "\n", comment )
    x <- gsub( re, rep,  x )
  }

  default_output_hook( x, options )

})

knitr::opts_template$set("kill_prefix"=list(comment=NA, null_prefix=TRUE))
```

\newpage

# Abstract {-}   

Recommender systems are one of the most successful and widespread application of machine learning technologies in business. There are a subsclass of information filtering system that seek to predict the "rating" or "preference" a user would give to an item. One of the most famous success story of  the recommender system is the Netflix competition launched on october 2006. In 2009, at the end of the challenge , Netflix awarded a one million dollar prize to a [developer team](https://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/) for an algorithm that increased the accuracy of the company's recommendation engine by 10%. Many well-known recommendation algorithms, such as latent factor models, were popularized by the Netflix contest. The Netflix prize contest is become notable for its numerous contributions to the data science community. With the 10M version of movielens data , and following some ML techniques that went into the [winning solution](http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/) for the Netflix competition, our findings suggest that [Linear regression model with regularized movie and user effects](#rg), and [recommender engine using Matrix factorization with Stochastic Gradient descent](#rs) lead us to the smallest Root Mean Squared Error(RMSE), 0.8648170 and 0.7826226 ,respectively.


\bigbreak
\bigbreak
\bigbreak
\bigbreak
\vspace{1.5cm}

# Introduction 

According to Aggarwal(2016) , the recommendation problem may be formulated in various ways, among which the two main are as follows: The first approach , the "prediction version of problem" aims to predict the rating value for a  user-item combination. It is also referred to as "matrix completion problem". The second approach, the "ranking version of problem" seeks to recommend the top-k items for a particular user, or determine the top-k users to target for a particular item. 
  
For the movielens project, we use the [10M version of the MovieLens dataset](https://grouplens.org/datasets/movielens/10m/) generated by the GroupLens research lab.
We aim to create our own recommendation system using the "prediction version of problem". We are 
going to train our algorithms using the inputs in edx set to predict movie ratings in the validation set. RMSE will be used to evaluate how close our predictions are to the true values in the validation set.This approach will be studied and the features processed and analyzed with different Machine Learning techniques, focusing on regression models,  ensemble methods (gradient boosting, random forest )  and recommender engines ( slopeOne, matrix factorization with gradient descent) following the main steps of a data mining problem as described in Ricci et al (2015).


\pagebreak


# Dataset and executive summary

##Overview


```{r, echo=FALSE, warning=FALSE, message=FALSE }
# Load library 
library(tidyverse)
library(caret)
library(data.table)
options(kableExtra.latex.load_packages = FALSE)
library(kableExtra)
library(lubridate)
library(Matrix.utils)
library(recommenderlab)
library(irlba)
library(SlopeOne)
library(recosystem)
library(h2o)


#Load data
load("EnvCapstone1.RData")   #Load output of Capstone_Movielens_data.R

#or you can just replace it with the entire  Capstone_Movielens_data.R code 

```


```{r , echo=FALSE,null_prefix =TRUE}
'edx set'
glimpse(edx)

'validation set'
glimpse(validation)
```


The edx set is made of 6 features for a total of about 9,000,055 observations while the validation set which represents 10% of the 10M Movielens dataset contains the same features , but with a total of 999,999 occurences. we made sure that userId and movieId in edx set are also in validation set. Each row represents a rating given by one user to one movie. The column "rating" is the outcome we want to predict, y. 

Taking into account both datasets, here are the features and their characteristics:

\bigbreak

***quantitative features***

-*userId* : discrete, Unique ID for the user.

-*movieId*: discrete, Unique ID for the movie. 

-*timestamp* : discrete , Date and time the rating was given.


***qualitative features***

-*title*:  nominal , movie title (not unique)

-*genres*: nominal, genres associated with the movie.


***outcome,y***

-*rating* :  continuous, a rating between 0 and 5 for the movie. 


\bigbreak


##Data exploration 

The Data exploration step disclosed the following findings :

- ***Outcome(rating)*** : the average user's activity reveals that no user gives 0 as rating and half star ratings are less common than whole star ratings. The top 5 ratings from most to least are : 4, 3, 5, 3.5 and 2.

```{r, out.width='70%', echo = FALSE , fig.cap="\\label{fig:fig1}Number of ratings for each rating"}
#i create a dataframe "explore_ratings" which contains half star and whole star ratings  from the edx set : 
group <-  ifelse((edx$rating == 1 |edx$rating == 2 | edx$rating == 3 | 
                  edx$rating == 4 | edx$rating == 5) ,
                   "whole_star", 
                   "half_star") 

explore_ratings <- data.frame(edx$rating, group)

# histogram of ratings

ggplot(explore_ratings, aes(x= edx.rating, fill = group)) +
  geom_histogram( binwidth = 0.2) +
  scale_x_continuous(breaks=seq(0, 5, by= 0.5)) +
  scale_fill_manual(values = c("half_star"="purple", "whole_star"="brown")) +
  labs(x="rating", y="number of ratings", caption = "source data: edx set") +
  ggtitle("Histogram")

```


- ***qualitative features(genres,title)*** : visual exploration shows a strong evidence of a genre effect (Figure 2).Movies which have the highest number of ratings are in the top genres categories : Drama, Comedy or Action ( see details in the Rmd file report )

```{r out.width='70%' , echo=FALSE, fig.cap="\\label{fig:fig2}error bar plots by genres"}
# i take the original column "genre" from the edx set , whatever combination appears in this column .
# i compute the average and standard error for each "genre". i Plot these as error bar plots for genres with more than 100000 ratings.

edx %>% group_by(genres) %>%
  summarize(n = n(), avg = mean(rating), se = sd(rating)/sqrt(n())) %>%
  filter(n >= 100000) %>% 
  mutate(genres = reorder(genres, avg)) %>%
  ggplot(aes(x = genres, y = avg, ymin = avg - 2*se, ymax = avg + 2*se)) + 
  geom_point() +
  geom_errorbar() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "error bar plots by genres" , caption = "source data : edx set") +
  theme(
    panel.background = element_rect(fill = "lightblue",
                                    colour = "lightblue",
                                    size = 0.5, linetype = "solid"),
    panel.grid.major = element_line(size = 0.5, linetype = 'solid',
                                    colour = "white"), 
    panel.grid.minor = element_line(size = 0.25, linetype = 'solid',
                                    colour = "white")
  )
```

- ***quantitative features(movieId,userId,timestamp)*** : There's presence of movies effects and users effects, but also time effects ; some movies get rated more than others, and some users are more active than others at rating movies (Figure 3). The trend of the average ratings versus the date( timestamp in the datetime format with weekly time unit) shows some evidence of time effect (Figure 3). Since the number of uniques values for the userId is  69878 and for the movieId 10677 (that means that there are less movies provided for ratings than users that rated them), if we think in terms of a large matrix with user on the rows and movies on the columns, the challenges we face are the sparsity of our matrix and the curse of dimensionality.

```{r, echo=FALSE, warning=FALSE,message=FALSE}

unique_values <- data.frame(n=c("users","movies"),distinct_values=c(n_distinct(edx$userId), n_distinct(edx$movieId)))

kable(unique_values,"latex", align = "c") %>%
  kable_styling(bootstrap_options = "striped", full_width = F , position ="center") %>%
  column_spec(1,border_left = T) %>%
  column_spec(2,bold = T, border_right = T) %>%
  footnote(general = "unique values for userId and movieId",
           general_title = "Table 1:",
           footnote_as_chunk = T)
```


```{r, echo=FALSE, out.width='.49\\linewidth', fig.align="center", fig.show="hold", warning=FALSE, message=FALSE, fig.cap="\\label{fig:fig3}Movie,user and time effects"}


# histogram of number of ratings by movieId
#par(mfrow=c(1,3), mar=c(4,4,4,1), oma=c(0.5,0.5,0.5,0))
edx %>% 
  count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram( bins=30, color = "red") +
  scale_x_log10() + 
  ggtitle("Movies") +
  labs(subtitle  ="number of ratings by movieId", 
       x="movieId" , 
       y="number of ratings", 
       caption ="source data : edx set") +
  theme(panel.border = element_rect(colour="black", fill=NA)) 


# histogram of number of ratings by userId
edx %>% 
  count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram( bins=30, color = "gold") +
  scale_x_log10() + 
  ggtitle("Users") +
  labs(subtitle ="number of ratings by UserId", 
       x="userId" , 
       y="number of ratings",
       caption ="source data : edx set") +
  theme(panel.border = element_rect(colour="black", fill=NA)) 

# timestamp
edx %>% 
  mutate(date = round_date(as_datetime(timestamp), unit = "week")) %>%
  group_by(date) %>%
  summarize(rating = mean(rating)) %>%
  ggplot(aes(date, rating)) +
  geom_point() +
  geom_smooth() +
  ggtitle("Timestamp, time unit : week")+
  labs(subtitle = "average ratings",
       caption = "source data : edx set") + 
  theme(panel.border = element_rect(colour="black", fill=NA)) 

```


\vspace{1.5cm}


# Data Preprocessing


Real-life data typically needs to be preprocessed (e.g. cleansed, filtered, transformed) in order to be used by the machine learning techniques in the analysis step.

In this section, we focused mainly on data preprocessing techniques that are of particular importance when designing a Recommender system. These techniques include similarity measures (such as Euclidean distance, Cosine distance,etc) , sampling methods , and dimensionality reduction (such as PCA or SVD). We will use them when necessary.

First avoid, we are going to create the rating matrix . In the Data exploration step, we already higlighted the sparsity problem when considering a large matrix with users on the rows and movies on the columns.  Let's build effectively that matrix.



## Data transformation 

We used the SparseMatrix function of the Matrix package to consider that large matrix with users on the rows and movies on the column . In order to perform our transformation, we needed in a first step to encode the movieId and userId vectors as categories with the `as.factor()` function , and in a second step to convert userId & movieId into `numeric vectors` . These transformations were made on an edx.copy set since we wanted to keep unchanged our original training(edx) set (see code details in the rmd file report). The rating matrix() is a 69878 x 10677 rating matrix of class 'realRatingMatrix' with 9,000,055 ratings. Here , a look on the first 10 users of our sparse matrix.

```{r, echo=FALSE}
# As we said in the Data exploration step,  usersId and movieId should be treat as factors for some analysis purposes. To perform this transformation we make a copy of edx set, since we want to keep unchanged our original training set.(edx)

edx.copy <- edx

edx.copy$userId <- as.factor(edx.copy$userId)
edx.copy$movieId <- as.factor(edx.copy$movieId)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
# i used the SparseMatrix function . The output is a sparse matrix of class dgcMatrix.
# In order to use this function, i need to convert userId & movieId into numeric vectors.

edx.copy$userId <- as.numeric(edx.copy$userId)
edx.copy$movieId <- as.numeric(edx.copy$movieId)

sparse_ratings <- sparseMatrix(i = edx.copy$userId,
                         j = edx.copy$movieId ,
                         x = edx.copy$rating, 
                         dims = c(length(unique(edx.copy$userId)),
                                  length(unique(edx.copy$movieId))),  
                         dimnames = list(paste("u", 1:length(unique(edx.copy$userId)), sep = ""), 
                                        paste("m", 1:length(unique(edx.copy$movieId)), sep = "")))


# i can remove the copy created
rm(edx.copy)
```

```{r, echo=FALSE , message=FALSE, warning=FALSE}
#give a look on the first 10 users
sparse_ratings[1:10,1:10]


#Convert rating matrix into a recommenderlab sparse matrix
ratingMat <- new("realRatingMatrix", data = sparse_ratings)
```



## Similarity measures

More often, data mining techniques that are used for the modelling of different recommender systems approaches( collaborative filtering, content based, hybrid methods) are highly dependent on defining an appropriate similarity or distance measure.

To measure the similarity between users or between items, it is possible to use the following:
 
    + Minkowski Distance
    + Mahalanobis distance
    + Pearson correlation
    + Cosine similarity 
    
we will use the cosine similarity which is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. According to Ricci et al(2015), it is defined as follow : 

$$\cos(\pmb x, \pmb y) = \frac {\pmb x \cdot \pmb y}{||\pmb x|| \cdot ||\pmb y||}$$   

where $\cdot$ indicates vector dot product and ||x|| is the norm of vector x .

The main advantages using this distance measure , as developed in Saranya et al (2016) are : 

- Solves the problem of sparsity, scalability and cold start and it is more robust to noise.
- It improves prediction accuracy and consistency
- The Cosine similarity can still be calculated even though the matrix has many missing elements.
- As the dimensional space becomes large, this still works well.
- The low Computational complexity , especially for sparse vectors.

Because of our huge amount of data, we calculate our similarity on the first 50 users for the visualization.


```{r echo=FALSE, out.width='.49\\linewidth', fig.align="center", fig.show="hold", warning=FALSE, message=FALSE, fig.cap="\\label{fig:fig4}Users and Movies similarity"}
#i calculate the user similarity using the cosine similarity

similarity_users <- similarity(ratingMat[1:50,], 
                               method = "cosine", 
                               which = "users")

image(as.matrix(similarity_users), main = "Users similarity")



#Using the same approach, I compute similarity between  movies.

similarity_movies <- similarity(ratingMat[,1:50], 
                               method = "cosine", 
                               which = "items")

image(as.matrix(similarity_movies), main = "Movies similarity")

```

In the given matrices, each row and each column corresponds to a user, and each cell corresponds to the similarity between two users.The more red the cell is, the more similar two users are. Note that the diagonal is red, since it's comparing each user with itself. 

When we observe the two similarity matrices, they leave out the following plausible analysis : since there are more similar ratings between certain users than others, and more similar ratings between certain movies than others, we can evidence the existence of a group of users pattern or a group of movies pattern.  



## Dimension reduction

The analysis of the previous similarity matrices leads to the thought that it can exists users with similar ratings patterns and movies with similar rating patterns. However Sparsity and the curse of dimensionality  remain a recurent problem, and we have to deal with many NA too. Dimensionality reduction techniques such as "pca" and "svd" can help overcome these problem by transforming the original high-dimensional space into a lower-dimensionality.

To face the RAM memory problem, we are going to use the Irlba package, which it is a fast and memory-efficient way to compute a partial SVD. The augmented implicitly restarted Lanczos bidiagonalization algorithm (IRLBA) finds a few approximate largest  (or, optionally, smallest) singular values and corresponding singular vectors of a sparse or dense matrix using a [method of Baglama and Reichel](http://www.math.kent.edu/~reichel/publications/auglbd.pdf)

According to Irizarry,R 2018 *Matrix factorization*,github page,accessed 15 January 2019, <https://rafalab.github.io/dsbook/matrix-factorization.html> , the SVD tells us that we can decompose a  $N * P$  matrix  $Y$ with  $P < N$  as follow :$$Y = UDV^T$$, where   
       
         + U : orthogonal matrix of dimensions N x m 
         + D : diagonal matrix containing the singular values of the original matrix, m x m 
         + V : orthogonal matrix of dimensions m x P


```{r, out.width='70%', echo=FALSE,warning=FALSE,message=FALSE, fig.cap="\\label{fig:fig5}Singular values for User-Movie Matrix"}
set.seed(1)
Y <- irlba(sparse_ratings,tol=1e-4,verbose=TRUE,nv = 100, maxit = 1000)

# plot singular values
plot(Y$d, pch=20, col = "blue", cex = 1.5, xlab='Singular Value', ylab='Magnitude', 
     main = "Singular Values for User-Movie Matrix")
```


```{r, echo=FALSE,warning=FALSE,message=FALSE}
# calculate sum of squares of all singular values
allsingsq <- sum(Y$d^2)

# variability described by first 6, 12, and 20 singular values
first6<- sum(Y$d[1:6]^2)
first12 <- sum(Y$d[1:12]^2)
first20 <- sum(Y$d[1:20]^2)

var_descr <- data.frame(singular_values=c("first6","first12","first20"),variability_described=c(first6/allsingsq, first12/allsingsq,first20/allsingsq))

#names(data) <- gsub("_", "", names(data))    # remove
#names(var_descr) <- gsub("_", "\\_", names(var_descr)) # escape
names(var_descr) <- gsub("_", " ", names(var_descr))   # replace with space

names(var_descr)[2] <- paste0(names(var_descr)[2],
# That "latex" can be eliminated if defined in global
footnote_marker_symbol(1, "latex"))

kable(var_descr,"latex", align = "c", escape=F) %>%
  kable_styling(bootstrap_options = "striped", full_width = F , position ="center") %>%
  column_spec(1,bold=T, border_left = T) %>%
  column_spec(2,bold = T, border_right = T) %>%
  footnote(general = "variability described by first 6, 12, and 20 singular values",
           general_title ="Table 2:",
           symbol = "is obtained as sum of squares of the specific singular value over sum of squares of all singular values ",
           symbol_title = "Note:",
           footnote_as_chunk = T)

```

First six singular values explain more than half of the variability of the imputed ratings matrix, with the first dozen explaining nearly 70% and the first twenty explaining more than 75%. However,the goal is to identify the first k singular values whose squares sum to at least 90% of the total of the sums of the squares of all of the singular values.


```{r, out.width='70%', echo=FALSE,warning=FALSE,message=FALSE,fig.cap="\\label{fig:fig6}best k for dimensionality reduction "}

perc_vec <- NULL
for (i in 1:length(Y$d)) {
  perc_vec[i] <- sum(Y$d[1:i]^2) / allsingsq
}

plot(perc_vec, pch=20, col = "blue", cex = 1.5, xlab='Singular Value', ylab='% of Sum of Squares of Singular Values', main = "Choosing k for Dimensionality Reduction")
lines(x = c(0,100), y = c(.90, .90))
```

A plot of a running sum of squares for the singular values shows that the 90% hurdle is achieved using somewhere between 50 and 60 singular values. After calculating the length of the vector that remains from our running sum of squares after excluding any items within that vector that exceed 0.90 (see code details in the rmd file report), we noticed that k=55 will retain 90% of variability.


```{r, echo=FALSE,warning=FALSE,message=FALSE}
#To find the exact value of k, i calculate  the length of the vector that remains from our running sum of squares after excluding any items within that vector that exceed 0.90.

k = length(perc_vec[perc_vec <= .90])

#I get the decomposition of Y ; matrices U, D, and V accordingly:

U_k <- Y$u[, 1:k]
D_k <- Diagonal(x = Y$d[1:k])
V_k <- t(Y$v)[1:k, ]



best_k_var <- data.frame(Y_Decomposition=c("U_k","D_k","V_k"),matrix_dimension =c( '69878*55', '55*55', '55*10677'))

names(best_k_var) <- gsub("_", "\\_", names(best_k_var))   # replace with escape


kable(best_k_var,"latex", align = "c") %>%
  kable_styling(bootstrap_options = "striped", full_width = F , position ="center") %>%
  column_spec(1,bold=T, border_left = T) %>%
  column_spec(2,bold = T, border_right = T) %>%
  footnote(general = "SVD decomposition of Y",
           general_title = "Table 3:",
           footnote_as_chunk = T)
```

As we can see above (table 3), we now have a matrix $D_k$ of size 55 x 55, a matrix $U_k$ of size 69878 x 55, and a matrix $V_k$ of size 55 x 10677. Therefore, the total number of numeric values required to house these component matrices is  $(69878 * 55) + (55 * 55) + (55 * 10677) = 4,433,550$ . This represents an approximately 50.7% decrease in required storage relative to the original 9,000,055 entries.

Reducing the dimensionality, the RAM memory problem persisted. Thats why we needed to go further with another reduction technique. We selected the relevant data using the whole rating matrix.


## Relevant Data

We know that some users saw more movies than the others. So, instead of displaying some random users and movies, we should select the most relevant users and movies. Thus we visualize only the users who have seen many movies and the movies that have been seen by many users.To identify and select the most relevant users and movies, we follow these steps:

   a. Determine the minimum number of movies per user.
   b. Determine the minimum number of users per movie.
   c. Select the users and movies matching these criteria.
   
```{r,echo=FALSE,warning=FALSE,message=FALSE,null_prefix =TRUE}
#a.
min_n_movies <- quantile(rowCounts(ratingMat), 0.9)

'min_n_movies'
print(min_n_movies)

#b.
min_n_users <- quantile(colCounts(ratingMat), 0.9)
'min_n_users'
print(min_n_users)

#c.
ratings_movies <- ratingMat[rowCounts(ratingMat) > min_n_movies,
                            colCounts(ratingMat) > min_n_users]
'matching criteria: ratings_movies'
ratings_movies
```

we can notice that now, we have a rating matrix of 6978 distinct users x 1068 distinct movies, with 2,313,148 ratings .

The data preprocessing phase is usually not definitive because it requires a lot of attention and subsequent various explorations on the variables. It must be aimed at obtaining better predictive results and in this sense, the further phases of model evaluations can help us to understand which particular preprocessing approaches are actually indispensable or useful for a specific model purpose.

\vspace{1.5cm}

# Methods and Analysis

In this section, we are going to explain the methodology over differents Machine Learning algorithms we used , and present the metric for the model performance evaluation.

## Evaluated Algorithms


### Regression Models


- **Modelling effects**

As in Irizarry,R 2018 *Recommender systems*,github page,accessed 5 January 2019, <https://rafalab.github.io/dsbook/recommendation-systems.html>, we followed the same approach to build our linear regression models as the simplest possible recommendation systems. We started from considering the same rating for all movies and users with all the differences explained by random variation $Y_{u,i} = \mu + \varepsilon_{u,i}$  and thus, modelling successively the different effects. 



***movie effects*** : since we know that some movies are generally rated higher than others, we can augment our previous model by adding the term  $b_i$ to represent average ranking for movie $i$: $$Y_{u,i} = \mu + b_i + \varepsilon_{u,i}  (1)$$   where : 
  
°$\mu$ the “true” rating for all movies 

°$b_i$  bs effects or bias , movie-specific effect.

°$\varepsilon_{u,i}$ independent errors sampled from the same distribution centered at 0



***movie + user effects*** : We also know that some users are more active than others at rating movies. This implies that an additional improvement to our model may be : $$Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}  (2)$$   where :
        
° $\mu$ , $b_i$ , $\varepsilon_{u,i}$   are defined as in $(1)$ 

° $b_u$   user-specific effect
  
  
  

***movie + user + time effects*** . As in data exploration we showed some evidence of time effect, if we define with $d_{u,i}$  as the day for user’s $u$ rating of movie $i$ the new model is the following : $$Y_{u,i} = \mu + b_i + b_u + f(d_{u,i}) + \varepsilon_{u,i}  (3)$$   with $f$  a smooth function of $d_{u,i}$ . A further and detailed explanations of time effects are developped in [Koren(2009)](https://www.asc.ohio-state.edu/statistics/dmsl/GrandPrize2009_BPC_BellKor.pdf). For convenience in our study , we just converted timestamp to a datetime object and rounded it to the weekly unit. 



***movie + user + genre effects*** : The same approach with the genre effect leads to the following model , $$Y_{u,i} = \mu + b_i + b_u + \sum{k=1}^K x_{u,i} \beta_k + \varepsilon_{u,i}  (4)$$   where
$g_{u,i}$  is defined as the genre for user’s $u$   and $x^k_{u,i} = 1$ if $g_{u,i}$  is genre $k$. We just present the model here, but we didn't perform it in our analysis.
  
  
To fit these models and get least square estimates of $\hat{b}_i$ , $\hat{b}_u$ , $\hat{b}_t$ , using the **lm()** function of the stats package  will be very slow since there are more than 10,000 of ${b}_i$ effects and more that 60,000 ${b}_u$ effects. For the different models (1),(2),(3) we get estimates of least squares as follow : 
  
° $\hat{b}_i$   as the average of $y_{u,i} - \hat{\mu}$  for each movie i and the predicted ratings are : $\hat{y}_{u,i} = \hat{\mu} + \hat{b}_i$ $(1')$
        
° $\hat{b}_u$   as the average of $y_{u,i} - \hat{\mu} - \hat{b}_i$ and the predicted ratings are : $\hat{y}_{u,i} = \hat{\mu} + \hat{b}_i + \hat{b}_u$ $(2')$
        
° $\hat{b}_t$  as the average of $y_{u,i} - \hat{\mu} - \hat{b}_i -\hat{b}_u$  and the predicted ratings are : $\hat{y}_{u,i} = \hat{\mu} + \hat{b}_i + \hat{b}_u + \hat{b}_t$ $(3')$
    


- **Regularization**
      
  Regularization permits us to penalize large estimates that come from small sample sizes. It has commonalities with the Bayesian approach that shrunk predictions. The general idea is to add a penalty for large values of bi, bu  to the sum of squares equation that we minimize. So having many large bi or bu makes it harder to minimize.


A more accurate estimation of bu and bi will treat them symmetrically, by solving the least squares problem   $$\frac{1}{N} \sum_{u,i} \left(y_{i,u} - \mu - b_i - b_u \right)^2 + 
\lambda \left(\sum_{i} b_i^2 + \sum_{u} b_u^2\right) (5)$$  where  the first term , $\frac{1}{N} \sum_{u,i} \left(y_{i,u} - \mu - b_i - b_u \right)^2$, strives to find bu’s and bi’s that fit the given ratings. The regularizing term, $\lambda \left(\sum_{i} b_i^2 + \sum_{u} b_u^2\right)$, avoids overfitting by penalizing the magnitudes of the parameters.  This least square problem can be solved fairly efficiently by the method of stochastic gradient descent that will be object in the matrix factorization recommender engine.

At this step, we used cross validation to pick the optimal $\lambda$ (which corresponds to the minimum RMSE) and using calculus we can show that the values of $b_i$  and $b_u$ that minimize this equation are :
      
$$\hat{b}_i(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{\mu}\right)$$
$$\hat{b}_u(\lambda) = \frac{1}{\lambda + n_i} \sum_{u=1}^{n_i} \left(Y_{u,i} - \hat{\mu} - \hat{b}_i \right)$$
where $n_i$ is the number of ratings made for movie  $i$.       

When $n_i$ is very large, which will give us a stable estimate, then  $\lambda$ is effectively ignored since  $n_i+\lambda \approx n_i$ . However, when  $n_i$ is small, then the estimate $\hat{b}_i(\lambda)$  is shrunken towards 0 .The larger $\lambda$, the more we shrink.



### Recommender engines


- **Slope One**
      
  Slope One was introduced in a 2005 paper by Daniel Lemire and Anna Maclachlan , ‘Slope One Predictors for Online Rating-Based Collaborative Filtering’. This algorithm is one of the simplest way to perform collaborative filtering based on items’ similarity. This makes it very easy to implement and use, and accuracy of this algorithm equals to the accuracy of more complicated and resource-intensive algorithms. Slope One method operates with average differences of ratings between each item and makes predictions based on their weighted value. More details are given in  the said [paper](https://archipel.uqam.ca/317/1/lemiremaclachlan_sdm05.pdf).
  
  
To perform the SlopeOne method , we need to perform a specific pre-processing approach. We followed the different steps : 
  
i) create a copy of edx and validation set (edx.copy and valid.copy) ,retaining only userId, 
movieId and rating columns.
ii) change names and types of variables (to characters) for edx.copy and valid.copy dataset to
make them suitable for the SlopeOne package.
iii) Package SlopeOne works with data.table objects. It is pretty fast, but for huge dataset it 
needs a lot of RAM. Thats why we need to sample our edx.copy dataset. we create a Data partition index (a random sampling without replacement, with p = 0.1) to produce a small training set.
iv) The rest of the steps consist to normalize the ratings, build the Slope one model and finally make prediction on our validation set.
  
  
-  **Matrix Factorization with stochastic gradient Descent** 
   
   Matrix Factorization is a popular technique to solve recommender system problem. The main idea is to approximate the matrix $R_{m\times n}$ by the product of two matrixes of lower dimension, $P_{k\times m}$ and $Q_{k\times n}$ such that :  $$R\approx P'Q$$. Let $p_u$ be the u-th column of $P$, and $q_v$ be the v-th column of $Q$, then the rating given by user $u$ on item $v$ would be predicted as $p'_u q_v$. A typical solution for $P$ and $Q$ is given by the following regularization problem as defined in [Chin et al(2015)](https://www.csie.ntu.edu.tw/~cjlin/papers/libmf/libmf_journal.pdf)  : $$\min_{P,Q} \sum_{(u,v)\in R} \left[f(p_u,q_v;r_{u,v})+\mu_P||p_u||_1+\mu_Q||q_v||_1+\frac{\lambda_P}{2} ||p_u||_2^2+\frac{\lambda_Q}{2} ||q_v||_2^2\right]$$  where $(u,v)$ are locations of observed entries in $R$, $r_{u,v}$ is the observed rating, $f$ is the loss function, and $\mu_P$,$\mu_Q$,$\lambda_P$,$\lambda_Q$ are penalty parameters to avoid overfitting.

   Matrix P represents latent factors of users. So, each k-elements column of matrix P represents each user. Each k-elements column of matrix Q represents each item . So, to find rating for item i by user u we simply need to compute two vectors: P[,u]’ x Q[,i]. Further descriptions of this technique and the recosystem package are available [here](https://cran.r-project.org/web/packages/recosystem/vignettes/introduction.html).
   
   
To perform our recommender system using parallel Matrix factorization with stochastic gradient descent, we followed the different steps:
   
i) As in the SlopeOne method, we created an identical copy of edx and validation set (edx.copy and valid.copy) , selecting only userId, movieId and rating columns. However, with the recosystem package, the data file for training set needs to be arranged in sparse matrix triplet form, i.e., each line in the file contains three numbers "user_index", "item_index", "rating".
    
ii)  No RAM problem : Unlike most other R packages for statistical modeling that store the whole dataset and model object in memory, recosystem can significantly reduce memory use, for instance the constructed model that contains information for prediction can be stored in the hard disk, and output result can also be directly written into a file rather than be kept in memory. Thats why we simply use our whole edx.copy as train set (9,000,055 occurences) and valid.copy as validation set (999,999 occurences).
    
iii)  Finally, we create a model object by calling Reco() , call the tune() method to select best tuning parameters along a set of candidate values , train the model by calling the train() method and use the predict() method to compute predicted values.
   
   
In our study, before to perform SlopeOne and Matrix factorization, we also presented recommender algorithms of the recommenderlab package( UBCF, IBCF , Popular). However, we will not develop them here since they were not relevant for the evaluation through the root mean squared error(RMSE) on the validation set.Short description of these methods are available [here](https://cran.r-project.org/web/packages/recommenderlab/recommenderlab.pdf) or in this [kernel](https://rpubs.com/jeknov/movieRec).
   


### Ensemble Methods

-  **GBDT: Gradient Boosting decision trees**

Gradient Boosting Machine (for Regression and Classification) is a forward learning ensemble method. The guiding heuristic is that good predictive results can be obtained through increasingly refined approximations. H2O’s GBM sequentially builds regression trees on all the features of the dataset in a fully distributed way - each tree is built in parallel.

H2O’s Gradient Boosting Algorithms follow the algorithm specified by Hastie et al (2001):

Initialize $f_{k0} = 0, k=1,2,…,K$

For $m=1$ to $M$:

1.set $p_{k}(x)=\frac{e^{f_{k}(x)}}{\sum_{l=1}^{K}e^{f_{l}(x)}},k=1,2,…,K$

2.For $k=1$ to $K$:

  a. compute $r_{ikm}=y_{ik}-p_{k}(x_{i}),i=1,2,…,N$
  b. Fit a regression tree to the targets $r_{ikm},i=1,2,…,N$, giving terminal regions $R_{jim},j=1,2,…,J_{m}$
  c. compute $\gamma_{jkm}=\frac{K-1}{K} \frac{\sum_{x_{i} \in R_{jkm}}(r_{ikm})}{\sum_{x_{i} \in R_{jkm}}|r_{ikm}|(1-|r_{ikm})},j=1,2,…,J_m$
  d. Update $f_{km}(x)=f_{k,m-1}(x)+\sum_{j=1}^{J_m}\gamma_{jkm} I(x\in R_{jkm})$
    
Output $\hat{f_{k}}(x)=f_{kM}(x),k=1,2,…,K$



The gbm h2o function has many parameters. For example, nfolds, ntrees, max_depth or learn rate  which control cross-validation , max number of trees , maximum tree depth  and learning ratecan help to overcome overfitting. Instead, GBM’s parallel performance is strongly determined by the max_depth, nbins, nbins_cats parameters. In general, for datasets over 10GB, it makes sense to use 2 to 4 nodes; for datasets over 100GB, it makes sense to use over 10 nodes, and so on. Since GBDTs have a built-in ability to apply different methods to different slices of the data, we added in some predictors that help the trees make useful clusterings: not only factors vectors of users and movies, but also number of movies each user rated (**n.movies_byUser**) and number of users that rated each movie (**n.users_bymovie**).

After creating a copy of edx set with all features(same for the validation set), with the mutate function we added the n_m and n_u attributes, converted usersId and movieId into factors, split the data intro train and test , perform the gbm algorithm with some tunes on parameters, and finally make prediction on the validation set.

The next steps consisted to build :

- random forest model on an h2oFrame (using the same training set as in the gbm method)
- a [super learner or stacked ensemble](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html) using the best previous gbm and random forest models.


For more details on Machine learning h2o algorithms (Gradient boosting, random forest,etc) and the h2o package, follow [h2oalgorithms](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science.html) and [h2o package](https://cran.r-project.org/web/packages/h2o/h2o.pdf), respectively.



## Model performance evaluation

To assess our model performance , we seek to evaluate how close our predictions are to the true rating values in the validation set. For this, we take into account the Root Mean Square Error (RMSE).


To construct the RMSE, you first need to determine the residuals. Residuals are the difference between the actual values and the predicted values. I denoted them by  $\hat{y}_{u,i} -y_{u,i}$, where $y_{u,i}$ is the observed value for the ith observation and $\hat{y}_{u,i}$ is the predicted value.

They can be positive or negative as the predicted value under or over estimates the actual value. Squaring the residuals, averaging the squares, and taking the square root gives us the RMSE. We then use the RMSE as a measure of the spread of the y values about the predicted y value.


$$\mbox{RMSE} = \sqrt{\frac{1}{N} \sum_{u,i}^{} \left( \hat{y}_{u,i} - y_{u,i} \right)^2 }$$
```{r, echo=FALSE}
#i define the rmse function
RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
  }
```

\vspace{1.5cm}

# Results


## Identifying optimal model

### Regression Models {#rg}

```{r , echo=FALSE, warning=FALSE, message=FALSE}

#a.movie effect

# i calculate the average of all ratings of the edx set
mu <- mean(edx$rating)

# i calculate b_i on the training set
movie_avgs <- edx %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))

# predicted ratings
predicted_ratings_bi <- mu + validation %>% 
  left_join(movie_avgs, by='movieId') %>%
  .$b_i


#b.movie + user effect

#i calculate b_u using the training set 
user_avgs <- edx %>%  
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

#predicted ratings
predicted_ratings_bu <- validation %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred


#c.movie + user + time effect

#i create a copy of validation set , valid, and create the date feature which is the timestamp converted to a datetime object  and  rounded by week.

valid <- validation
valid <- valid %>%
  mutate(date = round_date(as_datetime(timestamp), unit = "week")) 

# i calculate time effects ( b_t) using the training set
temp_avgs <- edx %>%
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(date = round_date(as_datetime(timestamp), unit = "week")) %>%
  group_by(date) %>%
  summarize(b_t = mean(rating - mu - b_i - b_u))

# predicted ratings
  predicted_ratings_bt <- valid %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(temp_avgs, by='date') %>%
  mutate(pred = mu + b_i + b_u + b_t) %>%
  .$pred

#d.  i calculate the RMSE for movies, users and time effects 

rmse_model1 <- RMSE(validation$rating,predicted_ratings_bi)  
rmse_model2 <- RMSE(validation$rating,predicted_ratings_bu)
rmse_model3 <- RMSE(valid$rating,predicted_ratings_bt)
```


```{r, echo=FALSE,warning=FALSE}
#Before to proceed with regularization, i just remove the object copy of validation, "valid"
rm(valid)

#e. regularization 

# remembering (5), $\lambda$ is a tuning parameter. We can use cross-validation to choose it

lambdas <- seq(0, 10, 0.25)
  
  rmses <- sapply(lambdas, function(l){
    
    mu_reg <- mean(edx$rating)
    
    b_i_reg <- edx %>% 
      group_by(movieId) %>%
      summarize(b_i_reg = sum(rating - mu_reg)/(n()+l))
    
    b_u_reg <- edx %>% 
      left_join(b_i_reg, by="movieId") %>%
      group_by(userId) %>%
      summarize(b_u_reg = sum(rating - b_i_reg - mu_reg)/(n()+l))
    
    predicted_ratings_b_i_u <- 
      validation %>% 
      left_join(b_i_reg, by = "movieId") %>%
      left_join(b_u_reg, by = "userId") %>%
      mutate(pred = mu_reg + b_i_reg + b_u_reg) %>%
      .$pred
    
    return(RMSE(validation$rating,predicted_ratings_b_i_u))
  })
  
  
#For the full model, the optimal  λ is:
    
lambda <- lambdas[which.min(rmses)]

rmse_model4 <- min(rmses)
```

```{r, out.width='70%', echo=FALSE,warning=FALSE,message=FALSE,fig.cap="\\label{fig:fig7}Choosing the optimal lambda for regularization"}
df <- data.frame(lambdas,rmses)
#plot lambdas vs rmses : regularization
df %>% 
ggplot(aes(x=lambdas, y=rmses))+
geom_point()+
geom_segment(x=0, xend=lambda,y=rmse_model4,yend=rmse_model4, color="red")+
geom_segment(x=lambda, xend=lambda,y=0,yend=rmse_model4,color="red") +
annotate(geom="label",x = lambda,y = 0.86485,color=2, label=paste("x=",round(lambda,2),"\ny=",round(rmse_model4,7)))    
```

 
we can notice that with the movie and user effects combined, our RMSE decreased by almost 10% with respect to the only movie effect. The improvement when we add the time effect is not significant, (about a decrease of 0.011%). Then, we performed regularization (as explained in Methods and analysis section) using only the movie and user effects . With the optimal $\lambda = 5.25$ (figure 7) we observed that regularization gets down the RMSE's value to 0.8648170. RMSE's values for linear regression models with different adding effects and regularization are summarized in table 4.

```{r, echo=FALSE,warning=FALSE,message=FALSE}

#summarize all the rmse on validation set for Linear regression models

rmse_results <- data.frame(methods=c("movie effects","movie + user effects","movie + user + time effects", "Regularized Movie + User effects Model"),rmse = c(rmse_model1, rmse_model2,rmse_model3, rmse_model4))

names(rmse_results) <- gsub("_", "\\_", names(rmse_results))   # replace with escape

kable(rmse_results) %>%
  kable_styling(bootstrap_options = "striped" , full_width = F , position = "center") %>%
  kable_styling(bootstrap_options = "bordered", full_width = F , position ="center") %>%
  column_spec(1,bold = T,border_left = T) %>%
  column_spec(2,bold =T ,color = "white" , background ="#D7261E") %>%
  footnote(general = "RMSE values- Linear regression models",
           general_title = "Table 4:",
           footnote_as_chunk = T)

```



### Recommender engines {#rs}

We obtained the following rmse's values for Popular,ubcf and ibcf  methods of the recommenderlab package :
```{r,echo=FALSE,message=FALSE, null_prefix = TRUE}
# a. POPULAR , UBCF and IBCF algorithms of the recommenderlab package

model_pop <- Recommender(ratings_movies, method = "POPULAR", 
                      param=list(normalize = "center"))

#prediction example on the first 10 users
pred_pop <- predict(model_pop, ratings_movies[1:10], type="ratings")
#as(pred_pop, "matrix")[,1:10]

#Calculation of rmse for popular method 
set.seed(1)
e <- evaluationScheme(ratings_movies, method="split", train=0.7, given=-5)
#5 ratings of 30% of users are excluded for testing

model_pop <- Recommender(getData(e, "train"), "POPULAR")

prediction_pop <- predict(model_pop, getData(e, "known"), type="ratings")

rmse_popular <- calcPredictionAccuracy(prediction_pop, getData(e, "unknown"))[1]
'Popular method'
rmse_popular
#c("rmse_popular",round(rmse_popular,7))

#Estimating rmse for UBCF using Cosine similarity and selected n as 50 based on cross-validation
set.seed(1)
model <- Recommender(getData(e, "train"), method = "UBCF", 
                     param=list(normalize = "center", method="Cosine", nn=50))

prediction <- predict(model, getData(e, "known"), type="ratings")

rmse_ubcf <- calcPredictionAccuracy(prediction, getData(e, "unknown"))[1]
#c("rmse_ubcf",round(rmse_ubcf,7))
'ubcf method'
rmse_ubcf

#Estimating rmse for IBCF using Cosine similarity and selected n as 350 based on cross-validation
set.seed(1)

model <- Recommender(getData(e, "train"), method = "IBCF", 
                     param=list(normalize = "center", method="Cosine", k=350))

prediction <- predict(model, getData(e, "known"), type="ratings")

rmse_ibcf <- calcPredictionAccuracy(prediction, getData(e, "unknown"))[1]
#c("rmse_ibcf",round(rmse_ibcf,7))
'ibcf method'
rmse_ibcf
```

We observed that user-based CF  gives an ability to achieve lower RMSE on test set than Item-based CF and Popular methods. However, these methods don't fill with the scope of our study since the rmse evaluation is made on a test set after partitioning. We want to predict ratings for the 999999 rows of the validation set and then evaluate with RMSE how close these predictions are with respect to the true ratings values in validation set. Moreover, the new data in the predict function should be of a class "ratingMatrix". Validation set doesn't have to be modified. 

We then moved to the SlopeOne and Matrix factorization methods.



```{r, echo=FALSE,warning=FALSE,message=FALSE, null_prefix = TRUE}
#Before to perform SlopeOne method, i clear unusued memory
invisible(gc())
#knitr::knit_meta(class=NULL,clean=TRUE)

#b. SlopeOne  

# i create copy of training(edx) and validation sets where i retain only userId, movieId and rating
edx.copy <- edx %>%
            select(-c("genres","title","timestamp"))

valid.copy <- validation %>%
              select(-c("genres","title","timestamp"))

# i rename columns and convert them to characters  for edx.copy and valid.copy sets : item_id  is #seen as movie_id

names(edx.copy) <- c("user_id", "item_id", "rating")

edx.copy <- data.table(edx.copy)

edx.copy[, user_id := as.character(user_id)]
edx.copy[, item_id := as.character(item_id)]


names(valid.copy) <- c("user_id", "item_id", "rating")

valid.copy <- data.table(valid.copy)

valid.copy[, user_id := as.character(user_id)]
valid.copy[, item_id := as.character(item_id)]


#setkey() sorts a data.table and marks it as sorted (with an attribute sorted). The sorted columns are the key. The key can be any columns in any order. The columns are sorted in ascending order always. The table is changed by reference and is therefore very memory efficient.
setkey(edx.copy, user_id, item_id)
setkey(valid.copy, user_id, item_id)


#split data to create a small training sample ( to face the RAM memory issue)
set.seed(1)
idx <- createDataPartition(y = edx.copy$rating, times = 1, p = 0.1, list = FALSE)
edx.copy_train <- edx.copy[idx,]

#normalization
ratings_train_norm <- normalize_ratings(edx.copy_train)
```

```{r, echo=FALSE,warning=FALSE,message=FALSE, null_prefix = TRUE}
invisible(gc())
invisible(memory.limit(size = 56000))

#Building Slope One model:
#knitr::knit_meta(class=NULL,clean=TRUE)
model <- build_slopeone(ratings_train_norm$ratings)
```


```{r, echo=FALSE,warning=FALSE,message=FALSE, null_prefix = TRUE}
invisible(gc())
#Making predictions for valdation set:
  
predictions <- predict_slopeone(model, 
                                  valid.copy[ , c(1, 2), with = FALSE], 
                                  ratings_train_norm$ratings)

unnormalized_predictions <- unnormalize_ratings(normalized = ratings_train_norm, 
                                                ratings = predictions)

#rmse_slopeone <- sqrt(mean((unnormalized_predictions$predicted_rating - valid.copy$rating )^2))
rmse_slopeone <- RMSE( valid.copy$rating,unnormalized_predictions$predicted_rating) 

# i remove the created copies of sets
rm(edx.copy,valid_copy,edx.copy_train)

```

Performing the Matrix factorization with GD recommender method, we achieved a RMSE definitely lower than 0.8. There's an improvement (a decrease) of more than 23% with respect to the slopeOne method. (see results in table 5) . Note that the high rmse value for the SlopeOne method is also due to the small training set we used to face the RAM problem(see Methods and Analysis section - SlopeOne method). Instead, by having a larger training set, we give our algorithm a better chance of understanding the patterns in the set, rather than just learning to identify specific examples from the training set. This could save our time with validation in the long run, since we won't be dealing with quite so much overfitting.

```{r, echo=FALSE,warning=FALSE,message=FALSE, null_prefix = TRUE}
#Before to perform MF method, i clear unusued memory
invisible(gc())

#c. Matrix Factorization with parallel stochastic gradient descent

#i create a copy of training(edx) and validation sets where i retain only userId, movieId and rating features. i rename the three columns.

edx.copy <-  edx %>%
            select(-c("genres","title","timestamp"))

names(edx.copy) <- c("user", "item", "rating")


valid.copy <-  validation %>%
  select(-c("genres","title","timestamp"))

names(valid.copy) <- c("user", "item", "rating")


#as matrix
edx.copy <- as.matrix(edx.copy)
valid.copy <- as.matrix(valid.copy)


#write edx.copy and valid.copy tables on disk 
write.table(edx.copy , file = "trainset.txt" , sep = " " , row.names = FALSE, col.names = FALSE)
write.table(valid.copy, file = "validset.txt" , sep = " " , row.names = FALSE, col.names = FALSE)


#  data_file(): Specifies a data set from a file in the hard disk. 

set.seed(123) # This is a randomized algorithm
train_set <- data_file(system.file( "dat" ,"trainset.txt" , package = "recosystem"))
valid_set <- data_file(system.file( "dat" ,"validset.txt" , package = "recosystem"))


#Next step is to build Recommender object
r = Reco()


# tuning

'Matrix Factorization : tuning parameters with nfolds=5,dim=c(10,20,30),lrate=c(0.1,0.2),nthread=1'
 
'Penalty parameters to avoid overfitting : costp_l1 = 0,costq_l1 = 0,costp_l2 = 0.01,costq_l2 = 0.1'

opts = r$tune(train_set, opts = list(dim = c(10, 20, 30), lrate = c(0.1, 0.2),
                                     costp_l1 = 0, costq_l1 = 0,
                                     nthread = 1, niter = 10))
opts

#  trains the recommender model

' Matrix Factorization : trains the recommender model '

r$train(train_set, opts = c(opts$min, nthread = 1, niter = 20))

#Making prediction on validation set and calculating RMSE:

pred_file = tempfile()
```

```{r include=FALSE}
r$predict(valid_set, out_file(pred_file))  
```

```{r, echo=FALSE,warning=FALSE,message=FALSE, null_prefix = TRUE}
'Matrix Factorization : show first 10 predicted values'
print(scan(pred_file, n = 10))


#valid_set
scores_real <- read.table("validset.txt", header = FALSE, sep = " ")$V3
scores_pred <- scan(pred_file)

rm(edx.copy, valid.copy)

rmse_mf <- sqrt(mean((scores_real-scores_pred) ^ 2))
#0.7826226

#summarize all the rmse for recommender algorithms

rmse_results <- data.frame(methods=c("SlopeOne","Matrix factorization with GD"),rmse = c(rmse_slopeone, rmse_mf))

kable(rmse_results) %>%
  kable_styling(bootstrap_options = "striped" , full_width = F , position = "center") %>%
  kable_styling(bootstrap_options = "bordered", full_width = F , position ="center") %>%
  column_spec(1,bold = T , border_left = T) %>%
  column_spec(2,bold =T ,color = "white" , background ="#D7261E") %>%
   footnote(general = "RMSE values- recommender engines",
           general_title = "Table 5:",
           footnote_as_chunk = T)
  
```


### Ensemble Methods


We observed that the best gbdt and rf models (gbdt_3 and rf_3) have the lowest Root Mean Squared errors on the training set and produced rmse values on the validation set equals to 0.9839035 and 0.9543947, respectively . For the gbm model this happened when with 50 trees and max depth equals to 5, we only take into account the movieId and UserId (factor vectors) features, and with 3 folds cross validation .For the rf model , this happened when with 3 folds cross validation we trained  50 trees with max depth equals to 20 for the movieId and UserId (factor vectors) features. However, performance appears slower in RF than in GBM since  RF can go to depth 20, which can lead to up to 1+2+4+8+…+2^19 ~ 1M nodes to be split, and for every one of them, mtries=sqrt(4600)=67 columns need to be considered for splitting. Instead, GBM can go to depth 5, so only 1+2+4+8+16 = 31 nodes to be split, and for every one of them, all 4600 columns need to be considered. The rmse's values on validation set for the gbdt , rf and stacked ensemble models are summarized in table 6.

```{r,echo=FALSE,warning=FALSE,message=FALSE}
#knitr::knit_meta(class=NULL,clean=TRUE)
invisible(gc())


# i create a copy of edx set where i retain all the features
edx.copy <- edx

# i create new columns n_m (number of movies each user rated) and n_u(Number of users that rated each movie)  as defined in the Data analysis and method section
edx.copy <- edx.copy %>%
            group_by(userId) %>%
            mutate(n.movies_byUser = n())

edx.copy <- edx.copy %>%
  group_by(movieId) %>%
  mutate(n.users_bymovie = n())

# factor vectors of users and movies
edx.copy$userId <-  as.factor(edx.copy$userId)
edx.copy$movieId <- as.factor(edx.copy$movieId)

#i do the same for the validation set
valida.copy <- validation  

valida.copy <- valida.copy %>%
  group_by(userId) %>%
  mutate(n.movies_byUser = n())

valida.copy <- valida.copy %>%
  group_by(movieId) %>%
  mutate(n.users_bymovie = n())

valida.copy$userId  <- as.factor(valida.copy$userId)
valida.copy$movieId <- as.factor(valida.copy$movieId)
```

```{r include=FALSE}
#Attempts to start and/or connect to and H2O instance 
h2o.init(
 nthreads=-1,             
 max_mem_size = "10G")
#Remove all prior Clusters - remove in-memory objects to free up space
h2o.removeAll()
```


```{r ,echo=FALSE, warning=FALSE, message=FALSE}

#partitioning 
splits <- h2o.splitFrame(as.h2o(edx.copy), 
                         ratios = 0.7, 
                         seed = 1) 
train <- splits[[1]]
test <- splits[[2]]
```

```{r,echo=FALSE,warning=FALSE,message=FALSE}
h2o.no_progress()

#third gbm model :  ntrees = 50, max depth = 5, learn rate = 0.1 , nfolds = 3 

gbdt_3 <- h2o.gbm( x = c("movieId","userId") ,
           y = "rating" , 
           training_frame = train , 
           nfolds = 3,
           seed=1,
           keep_cross_validation_predictions = TRUE,
           fold_assignment = "Random") 


#Since the model gbdt_3  has the lower RMSE on training set,   

# i evaluate performance on test set
#h2o.performance(gbdt_3, test)

#i predict ratings on validation set and evaluate RMSE
pred.ratings.gbdt_3 <- h2o.predict(gbdt_3,as.h2o(valida.copy))

rmse_gbdt <- RMSE(pred.ratings.gbdt_3, as.h2o(valida.copy$rating))


#third rf model : ntrees = 50, max.depth = 20 , nfolds = 3
rf_3 <- h2o.randomForest(        
  training_frame = train,       
  x= c("movieId" ,"userId"),                      
  y= "rating", 
  nfolds=3,
  seed=1,
  keep_cross_validation_predictions = TRUE,
  fold_assignment = "Random"
)

#Since the model rf_3  has the lower RMSE on training set,   

# i evaluate performance on test set
#h2o.performance(rf_3, test)

#i predict ratings on validation set and evaluate RMSE
pred.ratings.rf_3 <- h2o.predict(rf_3,as.h2o(valida.copy))

rmse_rf <- RMSE(pred.ratings.rf_3, as.h2o(valida.copy$rating))


#stacked Ensemble : i take the best two previous model (gbdt_3 and rf_3)

ensemble <- h2o.stackedEnsemble(x = c("movieId" ,"userId"),
                                y = "rating",
                                training_frame = train,
                                model_id = "my_ensemble_auto",
                                base_models = list(gbdt_3@model_id, rf_3@model_id))

#i predict ratings on validation set and evaluate RMSE
pred.ratings.ensemble <- h2o.predict(ensemble,as.h2o(valida.copy))

rmse_ensemble <- RMSE(pred.ratings.ensemble, as.h2o(valida.copy$rating))


cat("RMSE for h2o gbdt model on the validation set:\nwe used the best gbdt model on training set (lowest rmse) -> gbdt_3;\nntrees = 50, max depth = 5, learn rate = 0.1, nfolds = 3, fold_assignment = Random,\n keep_cross_validation_predictions = TRUE,\n(see model details on rmd file report)" )


cat("RMSE for h2o rf model on the validation set:\nwe used the best rf model on training set (lowest rmse) -> rf_3;\nntrees = 50, max.depth = 20 , nfolds = 3, nbins=20 , fold_assignment = Random,\n keep_cross_validation_predictions = TRUE,\n(see model details on rmd file report)")

```


```{r,echo=FALSE,warning=FALSE,message=FALSE}
rmse_results <- data.frame(methods=c("gradient Boosting","random forest","stacked ensemble"),rmse = c(rmse_gbdt, rmse_rf, rmse_ensemble))

kable(rmse_results) %>%
  kable_styling(bootstrap_options = "striped" , full_width = F , position = "center") %>%
  kable_styling(bootstrap_options = "bordered", full_width = F , position ="center") %>%
  column_spec(1,bold = T , border_left = T) %>%
  column_spec(2,bold =T ,color = "white" , background ="#D7261E") %>%
   footnote(general = "RMSE values- Ensemble methods",
           general_title = "Table 6:",
           footnote_as_chunk = T)

# remove objects
rm(edx.copy,valida.copy)
```

```{r include=FALSE}
#close the cluster h2o

h2o.shutdown()
```



## Increasing model performance

Based on the differents RMSE's values we found for Linear regression models, recommenders algorithms and Ensemble models, we choose the two candidates which best predict ratings on the validation set : Linear regression model with Regularized movie + user effect (i) and Matrix factorization with stochastic gradient (ii).

We focused on optimizing the performance of (ii) using 10 fold cross validation ( nfolds = 10). This consisted to understand the behavior of the rmse value for the Matrix Factorization recommender engine when modifying values of number of latent factors (dim), gradient descend step rate (lrate), penalty parameters to avoid overfitting (cost) and number of threads for parallel computing (nthread).Figure 8 shows the number of latent factors vs. cross-validation RMSE . We noticed that the training model just needed 15 latent factors to reach a rmse value lower than 0.8. To sum up, we observed that even with these optimal criteria, the RMSE value for the Matrix factorization method is still below 0.8.(see value of rmse_mf_opt)


`r text_spec("Matrix factorization with stochastic gradient descent : Increasing performance", background = "#D05A6E", color = "white", bold = T)`

`r text_spec("optimization (in r$tune): nfolds = 10 ,niter = 50 ,lrate = 0.05 ,dim =c(1:20), nthread = 4", color = "red")`

`r text_spec("optimization (in r$train): niter = 100, nthread = 4", color = "red")`

`r text_spec("Penalty parameters(cost) to avoid overfitting remained unchanged" , color ="green")`


```{r,echo=FALSE,warning=FALSE,message=FALSE,null_prefix=TRUE}

#i create a copy of training(edx) and validation sets where i retain only userId, movieId and rating features. i rename the three columns.

edx.copy <-  edx %>%
            select(-c("genres","title","timestamp"))

names(edx.copy) <- c("user", "item", "rating")


valid.copy <-  validation %>%
  select(-c("genres","title","timestamp"))

names(valid.copy) <- c("user", "item", "rating")


#as matrix
edx.copy <- as.matrix(edx.copy)
valid.copy <- as.matrix(valid.copy)


#write edx.copy and valid.copy tables on disk 
write.table(edx.copy , file = "trainset.txt" , sep = " " , row.names = FALSE, col.names = FALSE)
write.table(valid.copy, file = "validset.txt" , sep = " " , row.names = FALSE, col.names = FALSE)


#data_file(): Specifies a data set from a file in the hard disk. 

set.seed(123) # This is a randomized algorithm
train_set <- data_file(system.file( "dat" ,"trainset.txt" , package = "recosystem"))
valid_set <- data_file(system.file( "dat" ,"validset.txt" , package = "recosystem"))


#Next step is to build Recommender object
r = Reco()


#Optimizing/tuning the recommender model
opts <- r$tune(train_set , opts = list(dim = c(1:20), lrate = c(0.05),
                                            nthread = 4 , costp_l1=0, 
                                            costq_l1 = 0,
                                            niter = 50, nfold = 10,
                                            verbose = FALSE))

#trains the recommender model
r$train(train_set, opts = c(opts$min , nthread = 4, niter = 100,verbose=FALSE))


#Making prediction on validation set and calculating RMSE:
pred_file = tempfile()
```

```{r include=FALSE}
r$predict(valid_set, out_file(pred_file))  
```


```{r,echo=FALSE,warning=FALSE,message=FALSE,null_prefix=TRUE}

'Matrix Factorization : show first 10 predicted values'
print(scan(pred_file, n = 10))


#valid_set
scores_real <- read.table("validset.txt", header = FALSE, sep = " ")$V3
scores_pred <- scan(pred_file)

# remove edx.copy and valid.copy objects
rm(edx.copy, valid.copy)

rmse_mf_opt <- sqrt(mean((scores_real-scores_pred) ^ 2))

'rmse_mf_opt'
rmse_mf_opt  

```


```{r, out.width='70%', echo=FALSE,warning=FALSE,message=FALSE,fig.cap="\\label{fig:fig8}number of latent factors vs. cross-validation RMSE"}

load("EnvCapstone_MatrixFacto_trainRmse.RData")

iter.line <- 15
tr_rmse.line <- mat.facto_rmse$tr_rmse[which(mat.facto_rmse$iter==15)]


mat.facto_rmse %>% 
  ggplot(aes(x=iter, y = tr_rmse))+
  geom_point(size= 5 , shape = 19 ) + 
  geom_smooth(aes(x= iter, y = tr_rmse)) +
  geom_segment(x=0,xend=iter.line ,y=tr_rmse.line,yend=tr_rmse.line, color="red", lty=2)+
  geom_segment(x=iter.line, xend=iter.line, y=0, yend=tr_rmse.line, color="red", lty=2) +
  annotate(geom="label",x = iter.line,y = 0.8350,color=2,     
           label=paste("x=",round(iter.line,0),"\ny=",round(tr_rmse.line ,4)))+
  labs(title="RMSE for different number of latent factors" ,
       caption = "based on the output of r$train(train_set, opts = c(opts$min, nthread = 4, niter = 100), \n show just first 30 iterations)"
  ) +
  ylab("RMSE") +
  xlab("Latent factors")
    
```

\vspace{1.5cm}

# Conclusion and suggestions

This MovieLens project has examined the potential best recommender system algorithm to predict movie ratings for the 10M version of the Movielens data.  Using the provided training set (edx) and validation set, we successively trained different linear regression models, recommender engines and Ensemble methods. The model evaluation performance through the RMSE ( root mean squared error) showed that the Linear regression model with regularized effects on users and movies, and the Matrix Factorization with Stochastic gradient descent are the two appropriate recommender systems to predict ratings on the validation set. Using 10 fold cross-validation to increase the performance of the Matrix Factorization method , in the sense of understanding the behavior of the rmse value with changing optimal criteria( dim, nthread, lrate, lim), we winded up a rmse value always less than 0.8. 
Future work includes further investigations on Ensemble methods in order to get lower RMSE values than the 0.9 order we got in Ensemble methods section. GBDT combine some advantages like including an ability to find non-linear transformations, ability to handle skewed variables without requiring transformations, computational robustness (e.g., highly collinear variables are not an issue) and high scalability. They also naturally lend themselves to parallelization. This has made them a good choice for several large scale practical problems such as ranking results of a search engine (Bellkor, 2009). Following this Bellkor winning solution, to get a rmse value less than 0.88 on Ensemble methods we should combine over 500 models adding not only the clustering effects of Number of movies each user rated, Number of users that rated each movie, but also hidden units of a restricted Boltzmann Machine.

\vspace{1.5cm}

# References {-}

* Aggarwal, C. C. (2016). Recommender systems (pp. 1-28). Cham: Springer International Publishing.

* Ricci, F., Rokach, L., & Shapira, B. (2015). Recommender systems: introduction and challenges. In Recommender systems handbook (pp. 1-34). Springer, Boston, MA.

* Irizzary,R., 2018,*Introduction to Data Science*,github page,https://rafalab.github.io/dsbook/ 

* Shani, G., & Gunawardana, A. (2011). Evaluating recommendation systems. In Recommender systems handbook (pp. 257-297). Springer, Boston, MA.

* Saranya, K. G., Sadasivam, G. S., & Chandralekha, M. (2016). Performance comparison of different similarity measures for collaborative filtering technique. Indian J. Sci. Technol, 9(29), 1-8.

* Koren, Y. (2009). The bellkor solution to the netflix grand prize. Netflix prize documentation, 81, 1-10.

* Chin, W. S., Zhuang, Y., Juan, Y. C., & Lin, C. J. (2015). A fast parallel stochastic gradient method for matrix factorization in shared memory systems. ACM Transactions on Intelligent Systems and Technology (TIST), 6(1), 2.

* Friedman, Jerome H. “Greedy Function Approximation: A Gradient Boosting Machine.” Annals of Statistics (2001): 1189-1232.

* Friedman, J., Hastie, T., & Tibshirani, R. (2001). The elements of statistical learning (Vol. 1, No. 10). New York: Springer series in statistics.

